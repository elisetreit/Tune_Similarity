{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86bcc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Dataset shape: (50473, 14)\n",
      "Vocabulary size: 64\n",
      "Number of unique tunes: 22027\n",
      "Number of unique tune types: 12\n",
      "Found 9305 tunes with at least 2 settings\n",
      "Created 32944 positive pairs and 65888 negative pairs\n"
     ]
    }
   ],
   "source": [
    " #Model Comparison for Tune Similarity\n",
    "# This notebook compares different models on tune similarity tasks using positive/negative sets\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import collections\n",
    "import random\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import warnings\n",
    "from tf_sentence_transformers import SentenceTransformer\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "## 1. Data Loading and Preparation\n",
    "\n",
    "# Load your tokenized dataset\n",
    "df = pd.read_pickle('/home/devcontainers/git/Tune_Similarity/notebooks/embedding/data/tokenized_dataset.pkl')\n",
    "\n",
    "# Load the vocabulary\n",
    "with open(\"/home/devcontainers/git/Tune_Similarity/notebooks/embedding/note_vocab.pkl\", \"rb\") as f:\n",
    "    vocab_list = pickle.load(f)\n",
    "\n",
    "# Create note lookup layer\n",
    "note_lookup = tf.keras.layers.StringLookup(\n",
    "    vocabulary=vocab_list,\n",
    "    mask_token=None,\n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Vocabulary size: {len(vocab_list)}\")\n",
    "print(f\"Number of unique tunes: {df.tune_id.nunique()}\")\n",
    "print(f\"Number of unique tune types: {df.type.nunique()}\")\n",
    "\n",
    "## 2. Create Positive and Negative Sets\n",
    "\n",
    "def create_positive_negative_sets(df: pd.DataFrame, \n",
    "                                 min_settings_per_tune: int = 2,\n",
    "                                 max_pairs_per_tune: int = 10,\n",
    "                                 negative_ratio: float = 2.0) -> Tuple[List, List]:\n",
    "    \"\"\"\n",
    "    Create positive pairs (same tune_id) and negative pairs (different tune_id, potentially different type)\n",
    "    \n",
    "    Returns:\n",
    "        positive_pairs: List of tuples (idx1, idx2) where both belong to same tune\n",
    "        negative_pairs: List of tuples (idx1, idx2) where they belong to different tunes\n",
    "    \"\"\"\n",
    "    # Group by tune_id\n",
    "    tune_groups = df.groupby('tune_id').groups\n",
    "    \n",
    "    # Filter tunes with at least min_settings_per_tune settings\n",
    "    valid_tunes = {tune_id: indices for tune_id, indices in tune_groups.items() \n",
    "                   if len(indices) >= min_settings_per_tune}\n",
    "    \n",
    "    print(f\"Found {len(valid_tunes)} tunes with at least {min_settings_per_tune} settings\")\n",
    "    \n",
    "    positive_pairs = []\n",
    "    negative_pairs = []\n",
    "    \n",
    "    # Create positive pairs (same tune, different settings)\n",
    "    for tune_id, indices in valid_tunes.items():\n",
    "        indices_list = list(indices)\n",
    "        # Create all possible pairs, but limit to max_pairs_per_tune\n",
    "        for i in range(len(indices_list)):\n",
    "            for j in range(i+1, len(indices_list)):\n",
    "                positive_pairs.append((indices_list[i], indices_list[j]))\n",
    "                if len(positive_pairs) % max_pairs_per_tune == 0:\n",
    "                    break\n",
    "            if len(positive_pairs) % max_pairs_per_tune == 0:\n",
    "                break\n",
    "    \n",
    "    # Create negative pairs (different tunes)\n",
    "    all_indices = [idx for indices in valid_tunes.values() for idx in indices]\n",
    "    tune_id_map = {idx: df.loc[idx, 'tune_id'] for idx in all_indices}\n",
    "    \n",
    "    target_negative_pairs = int(len(positive_pairs) * negative_ratio)\n",
    "    \n",
    "    while len(negative_pairs) < target_negative_pairs:\n",
    "        idx1, idx2 = random.sample(all_indices, 2)\n",
    "        if tune_id_map[idx1] != tune_id_map[idx2]:\n",
    "            negative_pairs.append((idx1, idx2))\n",
    "    \n",
    "    print(f\"Created {len(positive_pairs)} positive pairs and {len(negative_pairs)} negative pairs\")\n",
    "    \n",
    "    return positive_pairs, negative_pairs\n",
    "\n",
    "# Create the positive and negative sets\n",
    "positive_pairs, negative_pairs = create_positive_negative_sets(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e6f009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load up both oob model and custom embedding model\n",
    "oob_embed = SentenceTransformer.from_pretrained(\n",
    "    \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b47a3e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading failed: Exception encountered when calling Lambda.call().\n",
      "\n",
      "\u001b[1mWe could not automatically infer the shape of the Lambda's output. Please specify the `output_shape` argument for this Lambda layer.\u001b[0m\n",
      "\n",
      "Arguments received by Lambda.call():\n",
      "  • args=('<KerasTensor shape=(None, None), dtype=float32, sparse=False, ragged=False, name=durations>',)\n",
      "  • kwargs={'mask': 'None'}\n",
      "✗ Model loading failed\n"
     ]
    }
   ],
   "source": [
    "# Try this immediate fix for your current model\n",
    "def load_with_custom_objects(model_path):\n",
    "    \"\"\"\n",
    "    Load model with custom objects to handle Lambda layers\n",
    "    \"\"\"\n",
    "    \n",
    "    custom_objects = {\n",
    "        # Handle the Lambda layer functions\n",
    "        'lambda_2': lambda x: tf.expand_dims(x, -1),\n",
    "        'lambda_3': lambda z: tf.math.l2_normalize(z, axis=1),\n",
    "        \n",
    "        # If you have custom loss function\n",
    "        'loss': lambda y_true, y_pred: tf.constant(0.0),  # Dummy loss for loading\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        model = tf.keras.models.load_model(\n",
    "            model_path, \n",
    "            custom_objects=custom_objects, \n",
    "            compile=False  # Don't compile, just load architecture and weights\n",
    "        )\n",
    "        print(\"Model loaded successfully!\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Loading failed: {e}\")\n",
    "        return None\n",
    "\n",
    "# Try loading your model\n",
    "model_path = '/home/devcontainers/git/Tune_Similarity/notebooks/embedding/saved_models/tune_embedder_v0.keras'\n",
    "model = load_with_custom_objects(model_path)\n",
    "\n",
    "if model is not None:\n",
    "    print(\"✓ Model loaded successfully!\")\n",
    "    print(f\"Model summary:\")\n",
    "    model.summary()\n",
    "else:\n",
    "    print(\"✗ Model loading failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55113beb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
