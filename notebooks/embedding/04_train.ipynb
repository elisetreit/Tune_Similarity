{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e3a485c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 22:47:11.838758: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 22:47:11.852357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747892831.866211  177480 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747892831.869475  177480 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747892831.878527  177480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747892831.878542  177480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747892831.878543  177480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747892831.878544  177480 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 22:47:11.880682: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "from tensorflow.keras import layers, Model\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a92a09d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "df = pd.read_pickle('data/tokenized_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f0e1b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747892868.485926  177480 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9558 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# build the vocab\n",
    "all_notes = sorted({n for seq in df.note_seq for n in seq})\n",
    "note_lookup = tf.keras.layers.StringLookup(\n",
    "    vocabulary=all_notes,\n",
    "    mask_token=None,    # 0 will be reserved for padding\n",
    "    oov_token=\"[UNK]\"\n",
    ")\n",
    "df['note_ids'] = df.note_seq.apply(lambda seq: note_lookup(seq).numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0cfe77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep the dataset\n",
    "# 1) Build index by tune_id\n",
    "by_id = collections.defaultdict(list)\n",
    "for notes, durs, tid in zip(df.note_ids, df.dur_seq, df.tune_id):\n",
    "    by_id[int(tid)].append((notes, durs))\n",
    "tune_ids = list(by_id.keys())\n",
    "\n",
    "# 2) Generator that yields one (sample, label) at a time,\n",
    "#    but cycles in groups of batch_tunes * per_tune to ensure positives in each batch.\n",
    "def balanced_sample_generator(batch_tunes=16, per_tune=2):\n",
    "    while True:\n",
    "        chosen = random.sample(tune_ids, batch_tunes)\n",
    "        # build exactly batch_tunes * per_tune samples\n",
    "        for tid in chosen:\n",
    "            examples = random.choices(by_id[tid], k=per_tune)\n",
    "            for notes, durs in examples:\n",
    "                yield (notes, durs), tid\n",
    "\n",
    "# 3) Wrap it in a Dataset of *samples*, then pad into batches\n",
    "ds = tf.data.Dataset.from_generator(\n",
    "    balanced_sample_generator,\n",
    "    output_signature=(\n",
    "        (tf.TensorSpec(shape=(None,), dtype=tf.int32),   # notes sequence\n",
    "         tf.TensorSpec(shape=(None,), dtype=tf.float32)), # durs sequence\n",
    "        tf.TensorSpec(shape=(),   dtype=tf.int32)         # tune_id label\n",
    "    )\n",
    ").padded_batch(\n",
    "    batch_size=64,\n",
    "    padded_shapes=(\n",
    "        ([None], [None]),  # pad notes→[64, T], durs→[64, T]\n",
    "        []                 # labels→[64]\n",
    "    ),\n",
    "    padding_values=(\n",
    "        (0, 0.0),          # pad notes with 0, durs with 0.0\n",
    "        0                  # pad label (unused) with 0\n",
    "    )\n",
    ").prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3d6373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(embeddings):\n",
    "    # embeddings: [batch, dim]\n",
    "    # returns a [batch, batch] matrix of squared distances\n",
    "    dot = tf.matmul(embeddings, embeddings, transpose_b=True)\n",
    "    sq = tf.reduce_sum(tf.square(embeddings), axis=1, keepdims=True)\n",
    "    # d(i,j) = ||xi - xj||^2 = sq[i] - 2*dot[i,j] + sq[j]\n",
    "    return tf.maximum(sq - 2.0 * dot + tf.transpose(sq), 0.0)\n",
    "\n",
    "def batch_hard_triplet_loss(margin=0.3):\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        # y_true: [batch] int labels (tune_id)\n",
    "        # y_pred: [batch, dim] embeddings (already ℓ₂‐normalized or not)\n",
    "        labels = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n",
    "        embeddings = y_pred\n",
    "        # 1) Pairwise distance matrix\n",
    "        pdist = pairwise_distances(embeddings)\n",
    "        # 2) Masks for positive / negative pairs\n",
    "        labels_eq = tf.equal(tf.expand_dims(labels,1), tf.expand_dims(labels,0))  # [B,B]\n",
    "        mask_pos = tf.cast(labels_eq, tf.float32) - tf.eye(tf.shape(labels)[0])   # zero diagonal\n",
    "        mask_neg = 1.0 - tf.cast(labels_eq, tf.float32)\n",
    "\n",
    "        # 3) For each anchor i, hardest positive = max_{j!=i, same label} d(i,j)\n",
    "        hardest_pos = tf.reduce_max(pdist * mask_pos, axis=1)\n",
    "        # 4) For each anchor i, easiest negative = min_{k, different label} d(i,k)\n",
    "        #    to do that, add large constant to positives so they’re ignored in min()\n",
    "        max_dist = tf.reduce_max(pdist)\n",
    "        pdist_neg = pdist + max_dist * (1.0 - mask_neg)\n",
    "        hardest_neg = tf.reduce_min(pdist_neg, axis=1)\n",
    "\n",
    "        # 5) Combine with margin\n",
    "        tl = tf.maximum(hardest_pos - hardest_neg + margin, 0.0)\n",
    "        return tf.reduce_mean(tl)\n",
    "    return loss_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a19cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ durations           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ note_ids            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ durations[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ note_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,672</span> │ add_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_p… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ durations           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ note_ids            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ durations[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m2,112\u001b[0m │ note_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ time_distributed_1  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │         \u001b[38;5;34m64\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ add_1 (\u001b[38;5;33mAdd\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ time_distributed… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m12,672\u001b[0m │ add_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional_1[\u001b[38;5;34m…\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ global_average_p… │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,848</span> (58.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,848\u001b[0m (58.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,848</span> (58.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,848\u001b[0m (58.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "vocab_list = note_lookup.get_vocabulary()\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = len(vocab_list) + 2     # from your StringLookup (plus PAD & OOV)\n",
    "EMB_DIM    = 32\n",
    "RNN_UNITS = 32\n",
    "\n",
    "# 1) Define your two inputs\n",
    "notes_in = layers.Input(shape=(None,), dtype=\"int32\",   name=\"note_ids\")\n",
    "durs_in  = layers.Input(shape=(None,), dtype=\"float32\", name=\"durations\")\n",
    "\n",
    "# 2) Embed your notes (this is trainable, starts random)\n",
    "note_emb = layers.Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMB_DIM,\n",
    "    mask_zero=True,   # so padding=0 is ignored by downstream RNN\n",
    ")(notes_in)          # → shape (batch, timesteps, EMB_DIM)\n",
    "\n",
    "dur_feat = layers.Lambda(\n",
    "    lambda x: tf.expand_dims(x, -1),\n",
    "    mask=lambda inputs, mask: mask  # pass the incoming 2D mask straight through\n",
    ")(durs_in)\n",
    "# project durations into EMB_DIM via a Dense layer\n",
    "dur_emb = layers.TimeDistributed(layers.Dense(EMB_DIM))(dur_feat)\n",
    "# now both note_emb and dur_emb are (B, T, EMB_DIM)\n",
    "x = layers.Add()([note_emb, dur_emb])\n",
    "\n",
    "# → shape (batch, timesteps, EMB_DIM + 1)\n",
    "\n",
    "# 5) Encode with a Bidirectional GRU (return_sequences=True so we can pool)\n",
    "rnn_out = layers.Bidirectional(\n",
    "    layers.GRU(RNN_UNITS, return_sequences=True)\n",
    ")(x)\n",
    "# → shape (batch, timesteps, 2*RNN_UNITS)\n",
    "\n",
    "# 6) Pool across time (takes care of variable lengths & masks)\n",
    "tune_vec = layers.GlobalAveragePooling1D()(rnn_out)\n",
    "# → shape (batch, 2*RNN_UNITS)\n",
    "\n",
    "# 7) L2-normalize if you like (makes cosine‐based losses stable)\n",
    "tune_emb = layers.Lambda(lambda z: tf.math.l2_normalize(z, axis=1))(tune_vec)\n",
    "\n",
    "# 8) Build & compile\n",
    "model = Model(inputs=[notes_in, durs_in], outputs=tune_emb)\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=batch_hard_triplet_loss(margin=0.3)\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c39cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747893001.577172  178397 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7770/Unknown \u001b[1m348s\u001b[0m 45ms/step - loss: 0.0962"
     ]
    }
   ],
   "source": [
    "model.fit(ds, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f291fc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training…\n",
    "model.save(\"saved_models/tune_embedder_v0\")  \n",
    "# → creates a SavedModel directory you can reload anywhere\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0362fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
