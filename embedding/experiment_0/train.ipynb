{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9316f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Training Pipeline for Tune Embedding Experiments\n",
    "# This notebook template can be copied for each experiment\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import layers, models\n",
    "import collections\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "33967ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Experiment metadata\n",
    "EXPERIMENT_NAME = \"experiment_0\"  # Change this for each experiment\n",
    "EXPERIMENT_DESCRIPTION = \"Baseline model with triplet loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "86a4948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "CONFIG = {\n",
    "    \"experiment_name\": EXPERIMENT_NAME,\n",
    "    \"description\": EXPERIMENT_DESCRIPTION,\n",
    "    \"model_params\": {\n",
    "        \"emb_dim\": 32,\n",
    "        \"rnn_units\": 32,\n",
    "        \"dropout_rate\": 0.1,  # Add regularization\n",
    "        \"l2_reg\": 0.001       # L2 regularization\n",
    "    },\n",
    "    \"training_params\": {\n",
    "        \"batch_size\": 64,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"epochs\": 20,\n",
    "        \"margin\": 0.3,\n",
    "        \"batch_tunes\": 16,\n",
    "        \"per_tune\": 2\n",
    "    },\n",
    "    \"data_params\": {\n",
    "        \"min_tune_settings\": 2,\n",
    "        \"max_sequence_length\": 512\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a58a45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created directory: ../experiment_0\n",
      "✓ Created directory: ../experiment_0/checkpoints\n",
      "✓ Created directory: ../experiment_0/logs\n",
      "✓ Created directory: ../experiment_0/plots\n",
      "✓ Created directory: ../saved_models\n",
      "✓ Created directory: ../tokenized_data\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DIRECTORY SETUP\n",
    "# ============================================================================\n",
    "\n",
    "def setup_experiment_directories(experiment_name):\n",
    "    \"\"\"Create directory structure for experiment\"\"\"\n",
    "    \n",
    "    base_dir = Path(\"..\")\n",
    "    experiment_dir = base_dir / experiment_name\n",
    "    \n",
    "    # Create directories\n",
    "    dirs_to_create = [\n",
    "        experiment_dir,\n",
    "        experiment_dir / \"checkpoints\",\n",
    "        experiment_dir / \"logs\",\n",
    "        experiment_dir / \"plots\",\n",
    "        base_dir / \"saved_models\",\n",
    "        base_dir / \"tokenized_data\"\n",
    "    ]\n",
    "    \n",
    "    for dir_path in dirs_to_create:\n",
    "        dir_path.mkdir(parents=True, exist_ok=True)\n",
    "        print(f\"✓ Created directory: {dir_path}\")\n",
    "    \n",
    "    return experiment_dir\n",
    "experiment_dir = setup_experiment_directories(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3f7c7595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved config to: ../experiment_0/config.json\n"
     ]
    }
   ],
   "source": [
    "# Save experiment configuration\n",
    "config_path = experiment_dir / \"config.json\"\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"✓ Saved config to: {config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "46d540a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "def create_tune_embedding_model(vocab_size, config):\n",
    "    \"\"\"\n",
    "    Create tune embedding model with proper saving/loading support\n",
    "    \"\"\"\n",
    "    model_params = config[\"model_params\"]\n",
    "    \n",
    "    # Define inputs\n",
    "    notes_in = layers.Input(shape=(None,), dtype=\"int32\", name=\"note_ids\")\n",
    "    durs_in = layers.Input(shape=(None,), dtype=\"float32\", name=\"durations\")\n",
    "\n",
    "    # Note embeddings with regularization\n",
    "    note_emb = layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=model_params[\"emb_dim\"],\n",
    "        mask_zero=True,\n",
    "        embeddings_regularizer=tf.keras.regularizers.l2(model_params[\"l2_reg\"]),\n",
    "        name=\"note_embedding\"\n",
    "    )(notes_in)\n",
    "\n",
    "    # Duration features - using Reshape instead of Lambda for better saving\n",
    "    dur_feat = layers.Reshape((-1, 1), name=\"duration_reshape\")(durs_in)\n",
    "    \n",
    "    # Duration embedding with regularization\n",
    "    dur_emb = layers.TimeDistributed(\n",
    "        layers.Dense(\n",
    "            model_params[\"emb_dim\"], \n",
    "            kernel_regularizer=tf.keras.regularizers.l2(model_params[\"l2_reg\"]),\n",
    "            name=\"duration_dense\"\n",
    "        ),\n",
    "        name=\"duration_embedding\"\n",
    "    )(dur_feat)\n",
    "\n",
    "    # Combine embeddings\n",
    "    x = layers.Add(name=\"combine_embeddings\")([note_emb, dur_emb])\n",
    "    \n",
    "    # Add dropout for regularization\n",
    "    x = layers.Dropout(model_params[\"dropout_rate\"], name=\"embedding_dropout\")(x)\n",
    "\n",
    "    # Bidirectional GRU with regularization\n",
    "    rnn_out = layers.Bidirectional(\n",
    "        layers.GRU(\n",
    "            model_params[\"rnn_units\"], \n",
    "            return_sequences=True,\n",
    "            dropout=model_params[\"dropout_rate\"],\n",
    "            recurrent_dropout=model_params[\"dropout_rate\"],\n",
    "            kernel_regularizer=tf.keras.regularizers.l2(model_params[\"l2_reg\"]),\n",
    "            name=\"gru\"\n",
    "        ),\n",
    "        name=\"bidirectional_gru\"\n",
    "    )(x)\n",
    "\n",
    "    # Global average pooling\n",
    "    tune_vec = layers.GlobalAveragePooling1D(name=\"global_pooling\")(rnn_out)\n",
    "    \n",
    "    # Additional dense layer before normalization (optional)\n",
    "    tune_vec = layers.Dense(\n",
    "        2 * model_params[\"rnn_units\"],\n",
    "        activation='relu',\n",
    "        kernel_regularizer=tf.keras.regularizers.l2(model_params[\"l2_reg\"]),\n",
    "        name=\"dense_before_norm\"\n",
    "    )(tune_vec)\n",
    "    \n",
    "    tune_vec = layers.Dropout(model_params[\"dropout_rate\"], name=\"final_dropout\")(tune_vec)\n",
    "\n",
    "    # L2 normalization using custom layer instead of Lambda\n",
    "    class L2Normalize(layers.Layer):\n",
    "        def __init__(self, **kwargs):\n",
    "            super(L2Normalize, self).__init__(**kwargs)\n",
    "        \n",
    "        def call(self, inputs):\n",
    "            return tf.math.l2_normalize(inputs, axis=1)\n",
    "        \n",
    "        def get_config(self):\n",
    "            return super(L2Normalize, self).get_config()\n",
    "\n",
    "    tune_emb = L2Normalize(name=\"l2_normalize\")(tune_vec)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[notes_in, durs_in], \n",
    "        outputs=tune_emb, \n",
    "        name=f\"tune_embedder_{EXPERIMENT_NAME}\"\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "213674fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# LOSS FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def batch_hard_triplet_loss(margin=0.3):\n",
    "    \"\"\"Batch hard triplet loss\"\"\"\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        labels = tf.cast(tf.reshape(y_true, [-1]), tf.int32)\n",
    "        embeddings = y_pred\n",
    "        \n",
    "        # Pairwise distances\n",
    "        dot = tf.matmul(embeddings, embeddings, transpose_b=True)\n",
    "        sq = tf.reduce_sum(tf.square(embeddings), axis=1, keepdims=True)\n",
    "        pdist = tf.maximum(sq - 2.0 * dot + tf.transpose(sq), 0.0)\n",
    "        \n",
    "        # Masks\n",
    "        labels_eq = tf.equal(tf.expand_dims(labels,1), tf.expand_dims(labels,0))\n",
    "        mask_pos = tf.cast(labels_eq, tf.float32) - tf.eye(tf.shape(labels)[0])\n",
    "        mask_neg = 1.0 - tf.cast(labels_eq, tf.float32)\n",
    "\n",
    "        # Hard examples\n",
    "        hardest_pos = tf.reduce_max(pdist * mask_pos, axis=1)\n",
    "        max_dist = tf.reduce_max(pdist)\n",
    "        pdist_neg = pdist + max_dist * (1.0 - mask_neg)\n",
    "        hardest_neg = tf.reduce_min(pdist_neg, axis=1)\n",
    "\n",
    "        # Triplet loss\n",
    "        tl = tf.maximum(hardest_pos - hardest_neg + margin, 0.0)\n",
    "        return tf.reduce_mean(tl)\n",
    "    \n",
    "    return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9561b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "\n",
    "def load_training_data():\n",
    "    \"\"\"Load training and validation data for experiments\"\"\"\n",
    "    \n",
    "    print(\"📁 Loading training data...\")\n",
    "    \n",
    "    # Load splits\n",
    "    train_df = pd.read_pickle('../tokenized_data/train_dataset.pkl')\n",
    "    val_df = pd.read_pickle('../tokenized_data/val_dataset.pkl')\n",
    "    \n",
    "    # Load vocabulary\n",
    "    with open(\"../tokenized_data/note_vocab.pkl\", \"rb\") as f:\n",
    "        vocab_list = pickle.load(f)\n",
    "    \n",
    "    # Load split metadata for reference\n",
    "    with open(\"../tokenized_data/split_metadata.json\", \"r\") as f:\n",
    "        split_info = json.load(f)\n",
    "    \n",
    "    print(f\"✓ Train: {len(train_df):,} samples from {train_df.tune_id.nunique():,} tunes\")\n",
    "    print(f\"✓ Val:   {len(val_df):,} samples from {val_df.tune_id.nunique():,} tunes\")\n",
    "    print(f\"✓ Vocab: {len(vocab_list)} unique notes\")\n",
    "    \n",
    "    return train_df, val_df, vocab_list, split_info\n",
    "\n",
    "def load_test_data():\n",
    "    \"\"\"Load test data (only for final evaluation!)\"\"\"\n",
    "    \n",
    "    print(\"🔒 Loading test data for final evaluation...\")\n",
    "    \n",
    "    test_df = pd.read_pickle('../tokenized_data/test_dataset.pkl')\n",
    "    \n",
    "    with open(\"../tokenized_data/note_vocab.pkl\", \"rb\") as f:\n",
    "        vocab_list = pickle.load(f)\n",
    "    \n",
    "    print(f\"✓ Test: {len(test_df):,} samples from {test_df.tune_id.nunique():,} tunes\")\n",
    "    print(\"⚠️  Remember: Use test set only for final model comparison!\")\n",
    "    \n",
    "    return test_df, vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bd929d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# UPDATED TRAINING PIPELINE\n",
    "# ============================================================================\n",
    "\n",
    "def create_training_dataset_with_validation(train_df, val_df, config):\n",
    "    \"\"\"Create both training and validation datasets\"\"\"\n",
    "    \n",
    "    training_params = config[\"training_params\"]\n",
    "    \n",
    "    # Prepare training data (same as before but using train_df)\n",
    "    by_id = collections.defaultdict(list)\n",
    "\n",
    "    for notes, durs, tid in zip(train_df.note_ids, train_df.dur_seq, train_df.tune_id):\n",
    "        by_id[int(tid)].append((notes, durs))\n",
    "    \n",
    "    tune_ids = list(by_id.keys())\n",
    "    \n",
    "    def balanced_sample_generator():\n",
    "        while True:\n",
    "            chosen = random.sample(tune_ids, training_params[\"batch_tunes\"])\n",
    "            for tid in chosen:\n",
    "                examples = random.choices(by_id[tid], k=training_params[\"per_tune\"])\n",
    "                for notes, durs in examples:\n",
    "                    yield (notes, durs), tid\n",
    "    \n",
    "    # Training dataset\n",
    "    train_ds = tf.data.Dataset.from_generator(\n",
    "        balanced_sample_generator,\n",
    "        output_signature=(\n",
    "            (tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "             tf.TensorSpec(shape=(None,), dtype=tf.float32)),\n",
    "            tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "        )\n",
    "    ).padded_batch(\n",
    "        batch_size=training_params[\"batch_size\"],\n",
    "        padded_shapes=(([None], [None]), []),\n",
    "        padding_values=((0, 0.0), 0)\n",
    "    ).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    # Validation dataset (if val_df has multi-setting tunes)\n",
    "    val_ds = None\n",
    "    val_multi_setting = val_df.tune_id.value_counts()\n",
    "    val_multi_setting_tunes = val_multi_setting[val_multi_setting >= 2].index\n",
    "    \n",
    "    if len(val_multi_setting_tunes) > 0:\n",
    "        val_df_filtered = val_df[val_df.tune_id.isin(val_multi_setting_tunes)]\n",
    "        \n",
    "        val_by_id = collections.defaultdict(list)\n",
    "        for notes, durs, tid in zip(val_df_filtered.note_ids, val_df_filtered.dur_seq, val_df_filtered.tune_id):\n",
    "            val_by_id[int(tid)].append((notes, durs))\n",
    "        \n",
    "        val_tune_ids = list(val_by_id.keys())\n",
    "        \n",
    "        def val_generator():\n",
    "            # Generate a fixed set for validation (not infinite)\n",
    "            samples = []\n",
    "            for tid in val_tune_ids:\n",
    "                examples = random.choices(val_by_id[tid], k=2)  # 2 examples per tune\n",
    "                for notes, durs in examples:\n",
    "                    samples.append(((notes, durs), tid))\n",
    "            \n",
    "            # Shuffle once\n",
    "            random.shuffle(samples)\n",
    "            for sample in samples:\n",
    "                yield sample\n",
    "        \n",
    "        val_ds = tf.data.Dataset.from_generator(\n",
    "            val_generator,\n",
    "            output_signature=(\n",
    "                (tf.TensorSpec(shape=(None,), dtype=tf.int32),\n",
    "                 tf.TensorSpec(shape=(None,), dtype=tf.float32)),\n",
    "                tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "            )\n",
    "        ).padded_batch(\n",
    "            batch_size=training_params[\"batch_size\"],\n",
    "            padded_shapes=(([None], [None]), []),\n",
    "            padding_values=((0, 0.0), 0)\n",
    "        ).prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        print(f\"✓ Validation dataset created with {len(val_tune_ids)} tunes\")\n",
    "    else:\n",
    "        print(\"⚠️  No multi-setting tunes in validation set - skipping validation dataset\")\n",
    "    \n",
    "    return train_ds, val_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b50af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# CALLBACKS AND MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "def create_callbacks_with_validation(experiment_dir, config, use_validation=True):\n",
    "    \"\"\"Create callbacks, optionally including validation monitoring\"\"\"\n",
    "    \n",
    "    callbacks = []\n",
    "    \n",
    "    # Model checkpointing\n",
    "    if use_validation:\n",
    "        # Monitor validation loss if available\n",
    "        monitor_metric = 'val_loss'\n",
    "        mode = 'min'\n",
    "    else:\n",
    "        # Fall back to training loss\n",
    "        monitor_metric = 'loss'  \n",
    "        mode = 'min'\n",
    "    \n",
    "    # Best model checkpoint\n",
    "    best_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=experiment_dir / \"checkpoints\" / \"best_weights.weights.h5\",\n",
    "        save_weights_only=True,\n",
    "        save_best_only=True,\n",
    "        monitor=monitor_metric,\n",
    "        mode=mode,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(best_checkpoint)\n",
    "    \n",
    "    # Regular checkpoints\n",
    "    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=experiment_dir / \"checkpoints\" / \"weights_epoch_{epoch:02d}.weights.h5\",\n",
    "        save_weights_only=True,\n",
    "        save_freq='epoch',\n",
    "        verbose=0\n",
    "    )\n",
    "    callbacks.append(checkpoint_callback)\n",
    "    \n",
    "    # CSV logger\n",
    "    csv_logger = tf.keras.callbacks.CSVLogger(\n",
    "        experiment_dir / \"logs\" / \"training_log.csv\",\n",
    "        append=True\n",
    "    )\n",
    "    callbacks.append(csv_logger)\n",
    "    \n",
    "    # Learning rate reduction\n",
    "    lr_reducer = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=monitor_metric,\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(lr_reducer)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=monitor_metric,\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    callbacks.append(early_stopping)\n",
    "\n",
    "    similarity_callback = TuneSimilarityCallback(val_df)\n",
    "    callbacks.append(similarity_callback)\n",
    "    \n",
    "    return callbacks\n",
    "\n",
    "# ============================================================================\n",
    "# UPDATED EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "def create_evaluation_pairs(df, num_positive=500, num_negative=1000):\n",
    "    \"\"\"Create evaluation pairs for validation\"\"\"\n",
    "    \n",
    "    # Filter tunes with multiple settings\n",
    "    multi_setting_tunes = df.tune_id.value_counts()\n",
    "    multi_setting_tunes = multi_setting_tunes[multi_setting_tunes >= 2].index\n",
    "    \n",
    "    if len(multi_setting_tunes) == 0:\n",
    "        return [], []\n",
    "    \n",
    "    # Filter DataFrame to only include multi-setting tunes\n",
    "    df_filtered = df[df.tune_id.isin(multi_setting_tunes)]\n",
    "    \n",
    "    # Create positive pairs (same tune, different settings)\n",
    "    pos_pairs = []\n",
    "    for tune_id in multi_setting_tunes:\n",
    "        indices = df_filtered[df_filtered.tune_id == tune_id].index.tolist()\n",
    "        if len(indices) < 2:\n",
    "            continue\n",
    "        random.shuffle(indices)\n",
    "        for i in range(len(indices)):\n",
    "            for j in range(i + 1, len(indices)):\n",
    "                pos_pairs.append((indices[i], indices[j]))\n",
    "                if len(pos_pairs) >= num_positive:\n",
    "                    break\n",
    "            if len(pos_pairs) >= num_positive:\n",
    "                break\n",
    "        if len(pos_pairs) >= num_positive:\n",
    "            break\n",
    "    \n",
    "    # Create negative pairs (different tunes)\n",
    "    neg_pairs = []\n",
    "    all_indices = df_filtered.index.tolist()\n",
    "    random.shuffle(all_indices)\n",
    "    \n",
    "    for i in range(len(all_indices)):\n",
    "        for j in range(i + 1, len(all_indices)):\n",
    "            if df_filtered.iloc[all_indices[i]].tune_id != df_filtered.iloc[all_indices[j]].tune_id:\n",
    "                neg_pairs.append((all_indices[i], all_indices[j]))\n",
    "                if len(neg_pairs) >= num_negative:\n",
    "                    break\n",
    "        if len(neg_pairs) >= num_negative:\n",
    "            break\n",
    "    \n",
    "    return pos_pairs, neg_pairs\n",
    "def evaluate_on_validation(model, val_df):\n",
    "    \"\"\"Quick evaluation on validation set during training\"\"\"\n",
    "    \n",
    "    if len(val_df) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Create evaluation pairs\n",
    "    pos_pairs, neg_pairs = create_evaluation_pairs(\n",
    "        val_df, num_positive=min(500, len(val_df)//4), num_negative=min(1000, len(val_df)//2)\n",
    "    )\n",
    "    \n",
    "    if len(pos_pairs) == 0:\n",
    "        return {\"note\": \"No evaluation possible - insufficient multi-setting tunes\"}\n",
    "    \n",
    "    # Get embeddings for all required indices\n",
    "    all_indices = list(set([i for pair in pos_pairs + neg_pairs for i in pair]))\n",
    "    all_embeddings = {}\n",
    "    \n",
    "    for idx in all_indices:\n",
    "        notes = val_df.iloc[idx].note_ids\n",
    "        durs = val_df.iloc[idx].dur_seq\n",
    "        embedding = model.predict([\n",
    "            np.array([notes]), \n",
    "            np.array([durs])\n",
    "        ], verbose=0)[0]\n",
    "        all_embeddings[idx] = embedding\n",
    "    \n",
    "    # Calculate similarities\n",
    "    pos_sims = [\n",
    "        np.dot(all_embeddings[i], all_embeddings[j])\n",
    "        for i, j in pos_pairs\n",
    "    ]\n",
    "    \n",
    "    neg_sims = [\n",
    "        np.dot(all_embeddings[i], all_embeddings[j])\n",
    "        for i, j in neg_pairs\n",
    "    ]\n",
    "    \n",
    "    return {\n",
    "        \"val_positive_similarity\": np.mean(pos_sims),\n",
    "        \"val_negative_similarity\": np.mean(neg_sims),\n",
    "        \"val_separation\": np.mean(pos_sims) - np.mean(neg_sims)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuneSimilarityCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, val_df, eval_frequency=1):\n",
    "        super().__init__()\n",
    "        self.val_df = val_df\n",
    "        self.eval_frequency = eval_frequency\n",
    "        self.similarity_history = []\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if (epoch + 1) % self.eval_frequency == 0:\n",
    "            # Run similarity evaluation\n",
    "            metrics = evaluate_on_validation(self.model, self.val_df)\n",
    "            \n",
    "            # Log metrics\n",
    "            logs.update({\n",
    "                'val_pos_similarity': metrics['val_positive_similarity'],\n",
    "                'val_neg_similarity': metrics['val_negative_similarity'],\n",
    "                'val_separation': metrics['val_separation']\n",
    "            })\n",
    "            \n",
    "            self.similarity_history.append(metrics)\n",
    "            \n",
    "            # Print metrics\n",
    "            print(f\"\\nEpoch {epoch+1} Similarity Metrics:\")\n",
    "            print(f\"Positive: {metrics['val_positive_similarity']:.3f}\")\n",
    "            print(f\"Negative: {metrics['val_negative_similarity']:.3f}\")\n",
    "            print(f\"Separation: {metrics['val_separation']:.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "87c226fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SAVING AND LOADING FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def save_model_and_weights(model, experiment_dir, experiment_name):\n",
    "    \"\"\"Save model in multiple formats for reliability\"\"\"\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save weights to experiment directory\n",
    "    weights_path = experiment_dir / \"checkpoints\" / f\"final_weights_{timestamp}.weights.h5\"\n",
    "    model.save_weights(weights_path)\n",
    "    print(f\"✓ Saved weights to: {weights_path}\")\n",
    "    \n",
    "    # Save weights to main saved_models directory\n",
    "    main_weights_path = Path(\"../saved_models\") / f\"{experiment_name}.weights.h5\"\n",
    "    model.save_weights(main_weights_path)\n",
    "    print(f\"✓ Saved weights to: {main_weights_path}\")\n",
    "    \n",
    "    # Save model architecture\n",
    "    architecture_path = experiment_dir / \"model_architecture.json\"\n",
    "    with open(architecture_path, 'w') as f:\n",
    "        f.write(model.to_json())\n",
    "    print(f\"✓ Saved architecture to: {architecture_path}\")\n",
    "    \n",
    "    # Try to save full model (might fail with Lambda layers)\n",
    "    try:\n",
    "        model_path = experiment_dir / \"checkpoints\" / f\"full_model_{timestamp}\"\n",
    "        model.save(model_path, save_format='tf')\n",
    "        print(f\"✓ Saved full model to: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not save full model: {e}\")\n",
    "    \n",
    "    return weights_path, main_weights_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f11b112d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_experiment(experiment_name, vocab_size=None, checkpoint=\"best\"):\n",
    "    \"\"\"Load model from experiment directory\"\"\"\n",
    "    \n",
    "    experiment_dir = Path(experiment_name)\n",
    "    \n",
    "    # Load config\n",
    "    with open(experiment_dir / \"config.json\", 'r') as f:\n",
    "        config = json.load(f)\n",
    "    \n",
    "    # Recreate model architecture\n",
    "    if vocab_size is None:\n",
    "        # Try to infer vocab size from config or data\n",
    "        try:\n",
    "            with open(\"tokenized_data/note_vocab.pkl\", \"rb\") as f:\n",
    "                vocab_list = pickle.load(f)\n",
    "            vocab_size = len(vocab_list) + 2\n",
    "        except:\n",
    "            raise ValueError(\"Could not determine vocab_size. Please provide it explicitly.\")\n",
    "    \n",
    "    model = create_tune_embedding_model(vocab_size, config)\n",
    "    \n",
    "    # Load weights\n",
    "    if checkpoint == \"best\":\n",
    "        weights_path = experiment_dir / \"checkpoints\" / \"best_weights.h5\"\n",
    "    else:\n",
    "        weights_path = experiment_dir / \"checkpoints\" / f\"final_weights_{checkpoint}.h5\"\n",
    "    \n",
    "    if weights_path.exists():\n",
    "        model.load_weights(weights_path)\n",
    "        print(f\"✓ Loaded weights from: {weights_path}\")\n",
    "    else:\n",
    "        print(f\"⚠ Weights file not found: {weights_path}\")\n",
    "        return None\n",
    "    \n",
    "    return model, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "16ef1136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment_0\n",
      "==================================================\n",
      "📁 Loading training data...\n",
      "✓ Train: 35,415 samples from 15,418 tunes\n",
      "✓ Val:   7,509 samples from 3,304 tunes\n",
      "✓ Vocab: 64 unique notes\n",
      "✓ Data loaded\n",
      "✓ Validation dataset created with 1396 tunes\n",
      "✓ Model created\n",
      "✓ Model compiled\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"tune_embedder_experiment_0\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"tune_embedder_experiment_0\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ durations           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ note_ids            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ duration_reshape    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ durations[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Reshape</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ note_embedding      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,112</span> │ note_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ duration_embedding  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span> │ duration_reshape… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combine_embeddings  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ note_embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Add</span>)               │                   │            │ duration_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_dropout   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ combine_embeddin… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_gru   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">12,672</span> │ embedding_dropou… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_pooling      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bidirectional_gr… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_before_norm   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,160</span> │ global_pooling[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_dropout       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_before_nor… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l2_normalize        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ final_dropout[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">L2Normalize</span>)       │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ durations           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ note_ids            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ duration_reshape    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ durations[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mReshape\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ note_embedding      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │      \u001b[38;5;34m2,112\u001b[0m │ note_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ duration_embedding  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │         \u001b[38;5;34m64\u001b[0m │ duration_reshape… │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)   │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ combine_embeddings  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ note_embedding[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mAdd\u001b[0m)               │                   │            │ duration_embeddi… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_dropout   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ combine_embeddin… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bidirectional_gru   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │     \u001b[38;5;34m12,672\u001b[0m │ embedding_dropou… │\n",
       "│ (\u001b[38;5;33mBidirectional\u001b[0m)     │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_pooling      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ bidirectional_gr… │\n",
       "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_before_norm   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │      \u001b[38;5;34m4,160\u001b[0m │ global_pooling[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ final_dropout       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ dense_before_nor… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ l2_normalize        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ final_dropout[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mL2Normalize\u001b[0m)       │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,008</span> (74.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m19,008\u001b[0m (74.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">19,008</span> (74.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m19,008\u001b[0m (74.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Validation dataset created with 1396 tunes\n",
      "✓ Training dataset created\n",
      "✓ Callbacks created\n",
      "Steps per epoch: 553\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MAIN TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Starting {EXPERIMENT_NAME}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load data\n",
    "train_df, val_df, vocab_list, split_info = load_training_data()\n",
    "print(\"✓ Data loaded\")\n",
    "vocab_size = len(vocab_list) + 2\n",
    "# transform into datasets\n",
    "train_ds, val_ds = create_training_dataset_with_validation(\n",
    "    train_df, val_df, CONFIG\n",
    ")\n",
    "# Create model\n",
    "model = create_tune_embedding_model(vocab_size, CONFIG)\n",
    "print(\"✓ Model created\")\n",
    "\n",
    "# Compile model\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=CONFIG[\"training_params\"][\"learning_rate\"]\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=batch_hard_triplet_loss(margin=CONFIG[\"training_params\"][\"margin\"])\n",
    ")\n",
    "\n",
    "print(\"✓ Model compiled\")\n",
    "model.summary()\n",
    "\n",
    "# Create dataset\n",
    "train_ds, val_ds = create_training_dataset_with_validation(\n",
    "    train_df, val_df, CONFIG  # Using the same df for both train and val for simplicity\n",
    ")\n",
    "print(\"✓ Training dataset created\")\n",
    "\n",
    "# Create callbacks\n",
    "callbacks = create_callbacks_with_validation(\n",
    "    experiment_dir, CONFIG, use_validation=(val_ds is not None)\n",
    ")\n",
    "print(\"✓ Callbacks created\")\n",
    "steps_per_epoch = len(train_df) // CONFIG[\"training_params\"][\"batch_size\"]\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95de9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/20\n",
      "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.5068\n",
      "Epoch 1: val_loss improved from inf to 0.31430, saving model to ../experiment_0/checkpoints/best_weights.weights.h5\n",
      "\u001b[1m553/553\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1318s\u001b[0m 2s/step - loss: 0.5067 - val_loss: 0.3143 - learning_rate: 0.0010\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:34:07.994224: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "\t [[StatefulPartitionedCall/tune_embedder_experiment_0_1/duration_embedding_1/loop_body/MatMul/pfor/split/_42]]\n",
      "2025-06-03 18:34:07.994335: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 6591663739367850741\n",
      "2025-06-03 18:34:07.994340: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 1261855952135355151\n",
      "2025-06-03 18:34:07.994343: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 4658643603350602711\n",
      "2025-06-03 18:34:07.994345: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 7543694434847464891\n",
      "2025-06-03 18:34:07.994347: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 13475498626517158835\n",
      "2025-06-03 18:34:07.994350: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 18136982582040116473\n",
      "2025-06-03 18:34:07.994362: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 8468185507243360102\n",
      "2025-06-03 18:34:07.994365: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 3555299371824073420\n",
      "2025-06-03 18:34:07.994368: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 4342766467301137270\n",
      "2025-06-03 18:34:07.994369: I tensorflow/core/framework/local_rendezvous.cc:430] Local rendezvous send item cancelled. Key hash: 16056160902860413770\n",
      "2025-06-03 18:34:07.994387: I tensorflow/core/framework/local_rendezvous.cc:426] Local rendezvous recv item cancelled. Key hash: 14824617579426262318\n",
      "/home/devcontainers/miniconda3/envs/tf-gpu/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:151: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self._interrupted_warning()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m289/553\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m9:17\u001b[0m 2s/step - loss: 0.4015"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "print(\"Starting training...\")\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=CONFIG[\"training_params\"][\"epochs\"],\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0969a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "save_model_and_weights(model, experiment_dir, EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87199aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history_path = experiment_dir / \"training_history.pkl\"\n",
    "with open(history_path, 'wb') as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"✓ Saved training history to: {history_path}\")\n",
    "\n",
    "print(f\"✓ {EXPERIMENT_NAME} completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fc6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# history is the History object returned by model.fit(...)\n",
    "train_loss = history.history['loss']\n",
    "val_loss   = history.history.get('val_loss', None)\n",
    "epochs     = range(1, len(train_loss) + 1)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(epochs, train_loss,  label='Train Loss')\n",
    "if val_loss is not None:\n",
    "    plt.plot(epochs, val_loss,  label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
